{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to isntall jupyterlab in venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngpu = torch.cuda.device_count()\n",
    "ngpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '2.2.2+cu118'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2+cu118'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_name = torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2060 SUPER'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngpu = torch.cuda.device_count()\n",
    "gpu_infos = []\n",
    "mem = []\n",
    "if_gpu_ok = False\n",
    "ok_gpu_keywords={\"10\",\"16\",\"20\",\"30\",\"40\",\"A2\",\"A3\",\"A4\",\"P4\",\"A50\",\"500\",\"A60\",\"70\",\"80\",\"90\",\"M4\",\"T4\",\"TITAN\",\"L4\",\"4060\",\"H\",\"600\",\"506\",\"507\",\"508\",\"509\"}\n",
    "set_gpu_numbers=set()\n",
    "if torch.cuda.is_available() or ngpu != 0:\n",
    "    for i in range(ngpu):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        if any(value in gpu_name.upper()for value in ok_gpu_keywords):\n",
    "            # A10#A100#V100#A40#P40#M40#K80#A4500\n",
    "            if_gpu_ok = True  # 至少有一张能用的N卡\n",
    "            gpu_infos.append(\"%s\\t%s\" % (i, gpu_name))\n",
    "            set_gpu_numbers.add(i)\n",
    "            mem.append(int(torch.cuda.get_device_properties(i).total_memory/ 1024/ 1024/ 1024+ 0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.transliterate import romanize\n",
    "from pythainlp.util import normalize as thai_normalize\n",
    "from pythainlp.util import thai_to_eng, eng_to_thai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['สวัสดี', 'เท', 'ส', ' ', 'เท', 'ส']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word = 'สวัสดีเทส เทส'\n",
    "word_tokenize(test_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sattithesthes'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romanize(test_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'สวัสดีเทส เทส'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thai_normalize(test_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l;ylfugml gml'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thai_to_eng(test_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\CodeProject\\tts\\GPT-SoVITS-Experiment\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_id = 'clicknext/phayathaibert'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'ใครเป็นผู้รับ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# punctuation = ['!', '?', '…', \",\", \".\",\"@\"]#@是SP停顿\n",
    "punctuation = [\"!\", \"?\", \"…\", \",\", \".\", \"'\", \"-\", \"¿\", \"¡\"]\n",
    "pu_symbols = punctuation + [\"SP\", \"SP2\", \"SP3\", \"UNK\"]\n",
    "# pu_symbols = punctuation + [\"SP\", 'SP2', 'SP3','SP4', \"UNK\"]\n",
    "pad = \"_\"\n",
    "\n",
    "c = [\n",
    "    \"AA\",\n",
    "    \"EE\",\n",
    "    \"OO\",\n",
    "    \"b\",\n",
    "    \"c\",\n",
    "    \"ch\",\n",
    "    \"d\",\n",
    "    \"f\",\n",
    "    \"g\",\n",
    "    \"h\",\n",
    "    \"j\",\n",
    "    \"k\",\n",
    "    \"l\",\n",
    "    \"m\",\n",
    "    \"n\",\n",
    "    \"p\",\n",
    "    \"q\",\n",
    "    \"r\",\n",
    "    \"s\",\n",
    "    \"sh\",\n",
    "    \"t\",\n",
    "    \"w\",\n",
    "    \"x\",\n",
    "    \"y\",\n",
    "    \"z\",\n",
    "    \"zh\",\n",
    "]\n",
    "v = [\n",
    "    \"E1\",\n",
    "    \"En1\",\n",
    "    \"a1\",\n",
    "    \"ai1\",\n",
    "    \"an1\",\n",
    "    \"ang1\",\n",
    "    \"ao1\",\n",
    "    \"e1\",\n",
    "    \"ei1\",\n",
    "    \"en1\",\n",
    "    \"eng1\",\n",
    "    \"er1\",\n",
    "    \"i1\",\n",
    "    \"i01\",\n",
    "    \"ia1\",\n",
    "    \"ian1\",\n",
    "    \"iang1\",\n",
    "    \"iao1\",\n",
    "    \"ie1\",\n",
    "    \"in1\",\n",
    "    \"ing1\",\n",
    "    \"iong1\",\n",
    "    \"ir1\",\n",
    "    \"iu1\",\n",
    "    \"o1\",\n",
    "    \"ong1\",\n",
    "    \"ou1\",\n",
    "    \"u1\",\n",
    "    \"ua1\",\n",
    "    \"uai1\",\n",
    "    \"uan1\",\n",
    "    \"uang1\",\n",
    "    \"ui1\",\n",
    "    \"un1\",\n",
    "    \"uo1\",\n",
    "    \"v1\",\n",
    "    \"van1\",\n",
    "    \"ve1\",\n",
    "    \"vn1\",\n",
    "    \"E2\",\n",
    "    \"En2\",\n",
    "    \"a2\",\n",
    "    \"ai2\",\n",
    "    \"an2\",\n",
    "    \"ang2\",\n",
    "    \"ao2\",\n",
    "    \"e2\",\n",
    "    \"ei2\",\n",
    "    \"en2\",\n",
    "    \"eng2\",\n",
    "    \"er2\",\n",
    "    \"i2\",\n",
    "    \"i02\",\n",
    "    \"ia2\",\n",
    "    \"ian2\",\n",
    "    \"iang2\",\n",
    "    \"iao2\",\n",
    "    \"ie2\",\n",
    "    \"in2\",\n",
    "    \"ing2\",\n",
    "    \"iong2\",\n",
    "    \"ir2\",\n",
    "    \"iu2\",\n",
    "    \"o2\",\n",
    "    \"ong2\",\n",
    "    \"ou2\",\n",
    "    \"u2\",\n",
    "    \"ua2\",\n",
    "    \"uai2\",\n",
    "    \"uan2\",\n",
    "    \"uang2\",\n",
    "    \"ui2\",\n",
    "    \"un2\",\n",
    "    \"uo2\",\n",
    "    \"v2\",\n",
    "    \"van2\",\n",
    "    \"ve2\",\n",
    "    \"vn2\",\n",
    "    \"E3\",\n",
    "    \"En3\",\n",
    "    \"a3\",\n",
    "    \"ai3\",\n",
    "    \"an3\",\n",
    "    \"ang3\",\n",
    "    \"ao3\",\n",
    "    \"e3\",\n",
    "    \"ei3\",\n",
    "    \"en3\",\n",
    "    \"eng3\",\n",
    "    \"er3\",\n",
    "    \"i3\",\n",
    "    \"i03\",\n",
    "    \"ia3\",\n",
    "    \"ian3\",\n",
    "    \"iang3\",\n",
    "    \"iao3\",\n",
    "    \"ie3\",\n",
    "    \"in3\",\n",
    "    \"ing3\",\n",
    "    \"iong3\",\n",
    "    \"ir3\",\n",
    "    \"iu3\",\n",
    "    \"o3\",\n",
    "    \"ong3\",\n",
    "    \"ou3\",\n",
    "    \"u3\",\n",
    "    \"ua3\",\n",
    "    \"uai3\",\n",
    "    \"uan3\",\n",
    "    \"uang3\",\n",
    "    \"ui3\",\n",
    "    \"un3\",\n",
    "    \"uo3\",\n",
    "    \"v3\",\n",
    "    \"van3\",\n",
    "    \"ve3\",\n",
    "    \"vn3\",\n",
    "    \"E4\",\n",
    "    \"En4\",\n",
    "    \"a4\",\n",
    "    \"ai4\",\n",
    "    \"an4\",\n",
    "    \"ang4\",\n",
    "    \"ao4\",\n",
    "    \"e4\",\n",
    "    \"ei4\",\n",
    "    \"en4\",\n",
    "    \"eng4\",\n",
    "    \"er4\",\n",
    "    \"i4\",\n",
    "    \"i04\",\n",
    "    \"ia4\",\n",
    "    \"ian4\",\n",
    "    \"iang4\",\n",
    "    \"iao4\",\n",
    "    \"ie4\",\n",
    "    \"in4\",\n",
    "    \"ing4\",\n",
    "    \"iong4\",\n",
    "    \"ir4\",\n",
    "    \"iu4\",\n",
    "    \"o4\",\n",
    "    \"ong4\",\n",
    "    \"ou4\",\n",
    "    \"u4\",\n",
    "    \"ua4\",\n",
    "    \"uai4\",\n",
    "    \"uan4\",\n",
    "    \"uang4\",\n",
    "    \"ui4\",\n",
    "    \"un4\",\n",
    "    \"uo4\",\n",
    "    \"v4\",\n",
    "    \"van4\",\n",
    "    \"ve4\",\n",
    "    \"vn4\",\n",
    "    \"E5\",\n",
    "    \"En5\",\n",
    "    \"a5\",\n",
    "    \"ai5\",\n",
    "    \"an5\",\n",
    "    \"ang5\",\n",
    "    \"ao5\",\n",
    "    \"e5\",\n",
    "    \"ei5\",\n",
    "    \"en5\",\n",
    "    \"eng5\",\n",
    "    \"er5\",\n",
    "    \"i5\",\n",
    "    \"i05\",\n",
    "    \"ia5\",\n",
    "    \"ian5\",\n",
    "    \"iang5\",\n",
    "    \"iao5\",\n",
    "    \"ie5\",\n",
    "    \"in5\",\n",
    "    \"ing5\",\n",
    "    \"iong5\",\n",
    "    \"ir5\",\n",
    "    \"iu5\",\n",
    "    \"o5\",\n",
    "    \"ong5\",\n",
    "    \"ou5\",\n",
    "    \"u5\",\n",
    "    \"ua5\",\n",
    "    \"uai5\",\n",
    "    \"uan5\",\n",
    "    \"uang5\",\n",
    "    \"ui5\",\n",
    "    \"un5\",\n",
    "    \"uo5\",\n",
    "    \"v5\",\n",
    "    \"van5\",\n",
    "    \"ve5\",\n",
    "    \"vn5\",\n",
    "]\n",
    "\n",
    "v_without_tone = [\n",
    "    \"E\",\n",
    "    \"En\",\n",
    "    \"a\",\n",
    "    \"ai\",\n",
    "    \"an\",\n",
    "    \"ang\",\n",
    "    \"ao\",\n",
    "    \"e\",\n",
    "    \"ei\",\n",
    "    \"en\",\n",
    "    \"eng\",\n",
    "    \"er\",\n",
    "    \"i\",\n",
    "    \"i0\",\n",
    "    \"ia\",\n",
    "    \"ian\",\n",
    "    \"iang\",\n",
    "    \"iao\",\n",
    "    \"ie\",\n",
    "    \"in\",\n",
    "    \"ing\",\n",
    "    \"iong\",\n",
    "    \"ir\",\n",
    "    \"iu\",\n",
    "    \"o\",\n",
    "    \"ong\",\n",
    "    \"ou\",\n",
    "    \"u\",\n",
    "    \"ua\",\n",
    "    \"uai\",\n",
    "    \"uan\",\n",
    "    \"uang\",\n",
    "    \"ui\",\n",
    "    \"un\",\n",
    "    \"uo\",\n",
    "    \"v\",\n",
    "    \"van\",\n",
    "    \"ve\",\n",
    "    \"vn\",\n",
    "]\n",
    "\n",
    "# japanese\n",
    "ja_symbols = [\n",
    "    \"I\",\n",
    "    \"N\",\n",
    "    \"U\",\n",
    "    \"a\",\n",
    "    \"b\",\n",
    "    \"by\",\n",
    "    \"ch\",\n",
    "    \"cl\",\n",
    "    \"d\",\n",
    "    \"dy\",\n",
    "    \"e\",\n",
    "    \"f\",\n",
    "    \"g\",\n",
    "    \"gy\",\n",
    "    \"h\",\n",
    "    \"hy\",\n",
    "    \"i\",\n",
    "    \"j\",\n",
    "    \"k\",\n",
    "    \"ky\",\n",
    "    \"m\",\n",
    "    \"my\",\n",
    "    \"n\",\n",
    "    \"ny\",\n",
    "    \"o\",\n",
    "    \"p\",\n",
    "    \"py\",\n",
    "    \"r\",\n",
    "    \"ry\",\n",
    "    \"s\",\n",
    "    \"sh\",\n",
    "    \"t\",\n",
    "    \"ts\",\n",
    "    \"u\",\n",
    "    \"v\",\n",
    "    \"w\",\n",
    "    \"y\",\n",
    "    \"z\",\n",
    "    ###楼下2个留到后面加\n",
    "    # \"[\", #上升调型\n",
    "    # \"]\", #下降调型\n",
    "    # \"$\", #结束符\n",
    "    # \"^\", #开始符\n",
    "]\n",
    "\n",
    "arpa = {\n",
    "    \"AH0\",\n",
    "    \"S\",\n",
    "    \"AH1\",\n",
    "    \"EY2\",\n",
    "    \"AE2\",\n",
    "    \"EH0\",\n",
    "    \"OW2\",\n",
    "    \"UH0\",\n",
    "    \"NG\",\n",
    "    \"B\",\n",
    "    \"G\",\n",
    "    \"AY0\",\n",
    "    \"M\",\n",
    "    \"AA0\",\n",
    "    \"F\",\n",
    "    \"AO0\",\n",
    "    \"ER2\",\n",
    "    \"UH1\",\n",
    "    \"IY1\",\n",
    "    \"AH2\",\n",
    "    \"DH\",\n",
    "    \"IY0\",\n",
    "    \"EY1\",\n",
    "    \"IH0\",\n",
    "    \"K\",\n",
    "    \"N\",\n",
    "    \"W\",\n",
    "    \"IY2\",\n",
    "    \"T\",\n",
    "    \"AA1\",\n",
    "    \"ER1\",\n",
    "    \"EH2\",\n",
    "    \"OY0\",\n",
    "    \"UH2\",\n",
    "    \"UW1\",\n",
    "    \"Z\",\n",
    "    \"AW2\",\n",
    "    \"AW1\",\n",
    "    \"V\",\n",
    "    \"UW2\",\n",
    "    \"AA2\",\n",
    "    \"ER\",\n",
    "    \"AW0\",\n",
    "    \"UW0\",\n",
    "    \"R\",\n",
    "    \"OW1\",\n",
    "    \"EH1\",\n",
    "    \"ZH\",\n",
    "    \"AE0\",\n",
    "    \"IH2\",\n",
    "    \"IH\",\n",
    "    \"Y\",\n",
    "    \"JH\",\n",
    "    \"P\",\n",
    "    \"AY1\",\n",
    "    \"EY0\",\n",
    "    \"OY2\",\n",
    "    \"TH\",\n",
    "    \"HH\",\n",
    "    \"D\",\n",
    "    \"ER0\",\n",
    "    \"CH\",\n",
    "    \"AO1\",\n",
    "    \"AE1\",\n",
    "    \"AO2\",\n",
    "    \"OY1\",\n",
    "    \"AY2\",\n",
    "    \"IH1\",\n",
    "    \"OW0\",\n",
    "    \"L\",\n",
    "    \"SH\",\n",
    "}\n",
    "\n",
    "th_symbols = [\n",
    "    \"ก\", \"ข\", \"ฃ\", \"ค\", \"ฅ\", \"ฆ\", \"ง\", \"จ\", \"ฉ\", \"ช\", \"ซ\", \"ฌ\", \"ญ\", \"ฎ\", \"ฏ\", \"ฐ\", \"ฑ\", \"ฒ\", \"ณ\", \"ด\", \"ต\", \"ถ\", \"ท\", \"ธ\", \"น\", \"บ\", \"ป\", \"ผ\", \"ฝ\", \"พ\", \"ฟ\", \"ภ\", \"ม\", \"ย\", \"ร\", \"ล\", \"ว\", \"ศ\", \"ษ\", \"ส\", \"ห\", \"ฬ\", \"อ\", \"ฮ\", \"ะ\", \"ั\", \"า\", \"ำ\", \"ิ\", \"ี\", \"ึ\", \"ื\", \"ุ\", \"ู\", \"เ\", \"แ\", \"โ\", \"ใ\", \"ไ\", \"ๅ\", \"็\", \"่\", \"้\", \"์\", \"๑\", \"๒\", \"๓\", \"๔\", \"๕\", \"๖\", \"๗\", \"๘\", \"๙\", \"๐\", \"aj\", \"pʰ\", \"uː\", \"˥˩\", \"˦˥\", \"t͡ɕʰ\", \"aː\", \"˩˩˦\", \"tʰ\", \"eː\", \"k̚\", \"˨˩\", \"t͡ɕ\", \"aʔ\", \"iː\", \"ɤː\", \"t̚\", \"ɛːw\", \"ɯː\", \"ua̯\", \"ɛː\", \"aːj\", \"ua̯j\", \"ɤːj\", \"ɔː\", \"a̯\", \"oː\", \"aːw\", \"ɔːj\", \"oʔ\", \"lɯ\", \"ɯa̯\", \"ɛʔ\", \"ia̯w\", \"lɯː\", \"rɯː\", \"oːj\", \"ɔʔ\", \"ๆ\", \"ɔj\", \";\", \"ew\", \"ɤʔ\", \"iw\", \"๊\", \"”\", \"eʔ\", \"uj\", \"“\", \"๋\", \"ฤ\", \"eːw\", \"a̯j\", \"ɛw\", \"‘\", \"’\", \"—\", \"ia̯ʔ\", \"ํ\", \"ŋ\", \"˥\", \"ʰ\", \"ɗ\", \"ɛ\", \"ǐ\", \"ᵊ\", \"ì\", \"̯\", \"̚\", \"˦\", \"ɯ\", \"˩\", \"ɕ\", \"˨\", \"ɓ\", \"cʰ\", \"æ\", \"ɔ\", \"̌\", \"à\", \"ː\", \" \", \"ə\", \"æː\", \"i̯\", \"▁\", \"am\", \"aw\" \n",
    "]\n",
    "\n",
    "ko_symbols='ㄱㄴㄷㄹㅁㅂㅅㅇㅈㅊㅋㅌㅍㅎㄲㄸㅃㅆㅉㅏㅓㅗㅜㅡㅣㅐㅔ空停'\n",
    "# ko_symbols='ㄱㄴㄷㄹㅁㅂㅅㅇㅈㅊㅋㅌㅍㅎㄲㄸㅃㅆㅉㅏㅓㅗㅜㅡㅣㅐㅔ '\n",
    "\n",
    "yue_symbols={'Yeot3', 'Yip1', 'Yyu3', 'Yeng4', 'Yut5', 'Yaan5', 'Ym5', 'Yaan6', 'Yang1', 'Yun4', 'Yon2', 'Yui5', 'Yun2', 'Yat3', 'Ye', 'Yeot1', 'Yoeng5', 'Yoek2', 'Yam2', 'Yeon6', 'Yu6', 'Yiu3', 'Yaang6', 'Yp5', 'Yai4', 'Yoek4', 'Yit6', 'Yam5', 'Yoeng6', 'Yg1', 'Yk3', 'Yoe4', 'Yam3', 'Yc', 'Yyu4', 'Yyut1', 'Yiu4', 'Ying3', 'Yip3', 'Yaap3', 'Yau3', 'Yan4', 'Yau1', 'Yap4', 'Yk6', 'Yok3', 'Yai1', 'Yeot6', 'Yan2', 'Yoek6', 'Yt1', 'Yoi1', 'Yit5', 'Yn4', 'Yaau3', 'Yau4', 'Yuk6', 'Ys', 'Yuk', 'Yin6', 'Yung6', 'Ya', 'You', 'Yaai5', 'Yau5', 'Yoi3', 'Yaak3', 'Yaat3', 'Ying2', 'Yok5', 'Yeng2', 'Yyut3', 'Yam1', 'Yip5', 'You1', 'Yam6', 'Yaa5', 'Yi6', 'Yek4', 'Yyu2', 'Yuk5', 'Yaam1', 'Yang2', 'Yai', 'Yiu6', 'Yin4', 'Yok4', 'Yot3', 'Yui2', 'Yeoi5', 'Yyun6', 'Yyu5', 'Yoi5', 'Yeot2', 'Yim4', 'Yeoi2', 'Yaan1', 'Yang6', 'Yong1', 'Yaang4', 'Yung5', 'Yeon1', 'Yin2', 'Ya3', 'Yaang3', 'Yg', 'Yk2', 'Yaau5', 'Yut1', 'Yt5', 'Yip4', 'Yung4', 'Yj', 'Yong3', 'Ya1', 'Yg6', 'Yaau6', 'Yit3', 'Yun3', 'Ying1', 'Yn2', 'Yg4', 'Yl', 'Yp3', 'Yn3', 'Yak1', 'Yang5', 'Yoe6', 'You2', 'Yap2', 'Yak2', 'Yt3', 'Yot5', 'Yim2', 'Yi1', 'Yn6', 'Yaat5', 'Yaam3', 'Yoek5', 'Ye3', 'Yeon4', 'Yaa2', 'Yu3', 'Yim6', 'Ym', 'Yoe3', 'Yaai2', 'Ym2', 'Ya6', 'Yeng6', 'Yik4', 'Yot4', 'Yaai4', 'Yyun3', 'Yu1', 'Yoeng1', 'Yaap2', 'Yuk3', 'Yoek3', 'Yeng5', 'Yeoi1', 'Yiu2', 'Yok1', 'Yo1', 'Yoek1', 'Yoeng2', 'Yeon5', 'Yiu1', 'Yoeng4', 'Yuk2', 'Yat4', 'Yg5', 'Yut4', 'Yan6', 'Yin3', 'Yaa6', 'Yap1', 'Yg2', 'Yoe5', 'Yt4', 'Ya5', 'Yo4', 'Yyu1', 'Yak3', 'Yeon2', 'Yong4', 'Ym1', 'Ye2', 'Yaang5', 'Yoi2', 'Yeng3', 'Yn', 'Yyut4', 'Yau', 'Yaak2', 'Yaan4', 'Yek2', 'Yin1', 'Yi5', 'Yoe2', 'Yei5', 'Yaat6', 'Yak5', 'Yp6', 'Yok6', 'Yei2', 'Yaap1', 'Yyut5', 'Yi4', 'Yim1', 'Yk5', 'Ye4', 'Yok2', 'Yaam6', 'Yat2', 'Yon6', 'Yei3', 'Yyu6', 'Yeot5', 'Yk4', 'Yai6', 'Yd', 'Yg3', 'Yei6', 'Yau2', 'Yok', 'Yau6', 'Yung3', 'Yim5', 'Yut6', 'Yit1', 'Yon3', 'Yat1', 'Yaam2', 'Yyut2', 'Yui6', 'Yt2', 'Yek6', 'Yt', 'Ye6', 'Yang3', 'Ying6', 'Yaau1', 'Yeon3', 'Yng', 'Yh', 'Yang4', 'Ying5', 'Yaap6', 'Yoeng3', 'Yyun4', 'You3', 'Yan5', 'Yat5', 'Yot1', 'Yun1', 'Yi3', 'Yaa1', 'Yaap4', 'You6', 'Yaang2', 'Yaap5', 'Yaa3', 'Yaak6', 'Yeng1', 'Yaak1', 'Yo5', 'Yoi4', 'Yam4', 'Yik1', 'Ye1', 'Yai5', 'Yung1', 'Yp2', 'Yui4', 'Yaak4', 'Yung2', 'Yak4', 'Yaat4', 'Yeoi4', 'Yut2', 'Yin5', 'Yaau4', 'Yap6', 'Yb', 'Yaam4', 'Yw', 'Yut3', 'Yong2', 'Yt6', 'Yaai6', 'Yap5', 'Yik5', 'Yun6', 'Yaam5', 'Yun5', 'Yik3', 'Ya2', 'Yyut6', 'Yon4', 'Yk1', 'Yit4', 'Yak6', 'Yaan2', 'Yuk1', 'Yai2', 'Yik2', 'Yaat2', 'Yo3', 'Ykw', 'Yn5', 'Yaa', 'Ye5', 'Yu4', 'Yei1', 'Yai3', 'Yyun5', 'Yip2', 'Yaau2', 'Yiu5', 'Ym4', 'Yeoi6', 'Yk', 'Ym6', 'Yoe1', 'Yeoi3', 'Yon', 'Yuk4', 'Yaai3', 'Yaa4', 'Yot6', 'Yaang1', 'Yei4', 'Yek1', 'Yo', 'Yp', 'Yo6', 'Yp4', 'Yan3', 'Yoi', 'Yap3', 'Yek3', 'Yim3', 'Yz', 'Yot2', 'Yoi6', 'Yit2', 'Yu5', 'Yaan3', 'Yan1', 'Yon5', 'Yp1', 'Yong5', 'Ygw', 'Yak', 'Yat6', 'Ying4', 'Yu2', 'Yf', 'Ya4', 'Yon1', 'You4', 'Yik6', 'Yui1', 'Yaat1', 'Yeot4', 'Yi2', 'Yaai1', 'Yek5', 'Ym3', 'Yong6', 'You5', 'Yyun1', 'Yn1', 'Yo2', 'Yip6', 'Yui3', 'Yaak5', 'Yyun2'}\n",
    "\n",
    "# symbols = [pad] + c + v + ja_symbols + pu_symbols + list(arpa)+list(ko_symbols)#+list(yue_symbols)###直接这么加yue顺序乱了\n",
    "symbols = [pad] + c + v + ja_symbols + pu_symbols + list(arpa)\n",
    "symbols = sorted(set(symbols))\n",
    "# print(len(symbols))\n",
    "symbols+=[\"[\",\"]\"]##日文新增上升下降调型\n",
    "symbols+=sorted(list(ko_symbols))\n",
    "symbols+=sorted(list(yue_symbols))##新加的yue统一摆在后头#已查过开头加Y后没有重复，韩文显然不会重复\n",
    "symbols+=sorted(list(th_symbols)) #Add Thai symbols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Big: Just so it can run\n",
    "# from text.symbols2 import symbols, punctuation\n",
    "# from GPT_SoVITS.text.thai_dictionary import english_dictionary, etc_dictionary\n",
    "punctuation = [\"!\", \"?\", \"…\", \",\", \".\", \"'\", \"-\", \"¿\", \"¡\"]\n",
    "\n",
    "# https://github.com/myshell-ai/MeloTTS/pull/117\n",
    "etc_dictionary = {\n",
    "    \"1+1\": \"วันพลัสวัน\",\n",
    "    \"2+1\": \"ทูพลัสวัน\",\n",
    "    # TODO: Add more Thai-specific abbreviations or special cases\n",
    "}\n",
    "\n",
    "english_dictionary = {\n",
    "    \"IT\": \"ไอที\",\n",
    "    \"IQ\": \"ไอคิว\",\n",
    "    \"PC\": \"พีซี\",\n",
    "    \"CCTV\": \"ซีซีทีวี\",\n",
    "    \"SNS\": \"เอสเอ็นเอส\",\n",
    "    \"AI\": \"เอไอ\",\n",
    "    \"CEO\": \"ซีอีโอ\",\n",
    "    \"A\": \"เอ\",\n",
    "    \"B\": \"บี\",\n",
    "    \"C\": \"ซี\",\n",
    "    \"D\": \"ดี\",\n",
    "    \"E\": \"อี\",\n",
    "    \"F\": \"เอฟ\",\n",
    "    \"G\": \"จี\",\n",
    "    \"H\": \"เอช\",\n",
    "    \"I\": \"ไอ\",\n",
    "    \"J\": \"เจ\",\n",
    "    \"K\": \"เค\",\n",
    "    \"L\": \"แอล\",\n",
    "    \"M\": \"เอ็ม\",\n",
    "    \"N\": \"เอ็น\",\n",
    "    \"O\": \"โอ\",\n",
    "    \"P\": \"พี\",\n",
    "    \"Q\": \"คิว\",\n",
    "    \"R\": \"อาร์\",\n",
    "    \"S\": \"เอส\",\n",
    "    \"T\": \"ที\",\n",
    "    \"U\": \"ยู\",\n",
    "    \"V\": \"วี\",\n",
    "    \"W\": \"ดับเบิลยู\",\n",
    "    \"X\": \"เอ็กซ์\",\n",
    "    \"Y\": \"วาย\",\n",
    "    \"Z\": \"แซด\",\n",
    "}\n",
    "\n",
    "from num2words import num2words\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.transliterate import romanize\n",
    "from pythainlp.util import normalize as thai_normalize\n",
    "from pythainlp.util import thai_to_eng, eng_to_thai\n",
    "\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_with_dictionary(text, dic):\n",
    "    if any(key in text for key in dic.keys()):\n",
    "        pattern = re.compile(\"|\".join(re.escape(key) for key in dic.keys()))\n",
    "        return pattern.sub(lambda x: dic[x.group()], text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_english(text):\n",
    "\n",
    "    def fn(m):\n",
    "        word = m.group()\n",
    "        if word.upper() in english_dictionary:\n",
    "            return english_dictionary[word.upper()]\n",
    "        return \"\".join(english_dictionary.get(char.upper(), char) for char in word)\n",
    "\n",
    "    text = re.sub(r\"([A-Za-z]+)\", fn, text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !! Need to look more on this  2025-02-23 see msome UNK word get put out need to kno wwhy?\n",
    "def normalize(text):\n",
    "    text = text.strip()\n",
    "    text = thai_normalize(text)\n",
    "    text = normalize_with_dictionary(text, etc_dictionary)\n",
    "    text = re.sub(r\"\\d+\", lambda x: num2words(int(x.group()), lang=\"th\"), text)\n",
    "    text = normalize_english(text)\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_text = 'ใครเป็นผู้รับ'\n",
    "phs = []\n",
    "# tones = []\n",
    "ph_groups = []\n",
    "tokenized = tokenizer.tokenize(norm_text)\n",
    "# Group tokens\n",
    "for t in tokenized:\n",
    "    if t == '▁':\n",
    "        ph_groups.append([t])  # Add '▁' as its own group\n",
    "    else:\n",
    "        if not ph_groups or ph_groups[-1] == ['▁']:\n",
    "            ph_groups.append([t])\n",
    "        else:\n",
    "            ph_groups[-1].append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = [\"!\", \"?\", \"…\", \",\", \".\", \"'\", \"-\", \"¿\", \"¡\"]\n",
    "\n",
    "word2ph = []\n",
    "for group in ph_groups:\n",
    "    text = \"\".join(group)\n",
    "    if text == '▁':\n",
    "        phs += ['_']\n",
    "        # tones += [0]  # Zero tone for underscore\n",
    "        word2ph += [1]\n",
    "        continue\n",
    "    elif text == '[UNK]':\n",
    "        phs += ['_']\n",
    "        # tones += [0]  # Zero tone for unknown token\n",
    "        word2ph += [1]\n",
    "        continue\n",
    "    elif text in punctuation:\n",
    "        phs += [text]\n",
    "        # tones += [0]  # Zero tone for punctuation\n",
    "        word2ph += [1]\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def map_word_to_phonemes(word):\n",
    "    logger.debug(f\"Looking up word: {word}\")\n",
    "\n",
    "    # First, try to find the whole word in the dictionary\n",
    "    phonemes_list = thai_g2p_dict.get(word)\n",
    "    if phonemes_list:\n",
    "        logger.debug(f\"Found whole word {word} in dictionary\")\n",
    "        return \" \".join(phonemes_list[0])\n",
    "\n",
    "    # If not found, try to split the word\n",
    "    subwords = word_tokenize(word, engine=\"newmm\")\n",
    "\n",
    "    if len(subwords) > 1:\n",
    "        logger.debug(f\"Word {word} split into subwords: {subwords}\")\n",
    "        # If the word can be split, recursively process each subword\n",
    "        return \" . \".join(map_word_to_phonemes(subword) for subword in subwords)\n",
    "    else:\n",
    "        logger.debug(f\"Word {word} cannot be split, processing character by character\")\n",
    "        return map_partial_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def thai_text_to_phonemes(text):\n",
    "    text = normalize(text)\n",
    "    words = word_tokenize(text, engine=\"newmm\")\n",
    "    phonemes = []\n",
    "    for word in words:\n",
    "        word_phonemes = map_word_to_phonemes(word)\n",
    "        phonemes.extend(word_phonemes.split())\n",
    "\n",
    "    # Map any remaining Thai characters\n",
    "    mapped_phonemes = map_remaining_thai_chars(phonemes)\n",
    "\n",
    "    return \" \".join(mapped_phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 21 (2777691680.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[23], line 22\u001b[1;36m\u001b[0m\n\u001b[1;33m    phs = [\"_\"] + phs + [\"_\"]\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'if' statement on line 21\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer.tokenize(norm_text)\n",
    "phs = []\n",
    "# tones = []\n",
    "ph_groups = []\n",
    "\n",
    "# Group tokens\n",
    "for t in tokenized:\n",
    "    if t == '▁':\n",
    "        ph_groups.append([t])  # Add '▁' as its own group\n",
    "    else:\n",
    "        if not ph_groups or ph_groups[-1] == ['▁']:\n",
    "            ph_groups.append([t])\n",
    "        else:\n",
    "            ph_groups[-1].append(t)\n",
    "\n",
    "word2ph = []\n",
    "for group in ph_groups:\n",
    "    text = \"\".join(group)\n",
    "    if text == '▁':\n",
    "        phs += ['_']\n",
    "        # tones += [0]  # Zero tone for underscore\n",
    "        word2ph += [1]\n",
    "        continue\n",
    "    elif text == '[UNK]':\n",
    "        phs += ['_']\n",
    "        # tones += [0]  # Zero tone for unknown token\n",
    "        word2ph += [1]\n",
    "        continue\n",
    "    elif text in punctuation:\n",
    "        phs += [text]\n",
    "        # tones += [0]  # Zero tone for punctuation\n",
    "        word2ph += [1]\n",
    "        continue\n",
    "\n",
    "    # Phoneme conversion for grouped text\n",
    "    phonemes = thai_text_to_phonemes(text)\n",
    "    phoneme_groups = phonemes.split(\".\")\n",
    "    phoneme_groups = list(filter(str.strip, phoneme_groups))\n",
    "\n",
    "    word_phonemes = []\n",
    "    # word_tones = []\n",
    "    for p_group in phoneme_groups:\n",
    "        group_phonemes, group_tones = extract_tones(p_group)\n",
    "        word_phonemes.extend(group_phonemes)\n",
    "        # word_tones.extend(group_tones)\n",
    "\n",
    "    phone_len = len(word_phonemes)\n",
    "    word_len = len(group)\n",
    "    aaa = distribute_phone(phone_len, word_len)\n",
    "    # assert len(aaa) == word_len\n",
    "    word2ph += aaa\n",
    "    # phs += word_phonemes\n",
    "    # tones += word_tones\n",
    "\n",
    "if pad_start_end:\n",
    "    phs = [\"_\"] + phs + [\"_\"]\n",
    "    # tones = [0] + tones + [0]  # Zero tone for start/end padding\n",
    "    word2ph = [1] + word2ph + [1]\n",
    "\n",
    "# assert len(phs) == len(tones)\n",
    "# assert len(phs) == sum(word2ph)\n",
    "\n",
    "# return phs, tones, word2ph\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
