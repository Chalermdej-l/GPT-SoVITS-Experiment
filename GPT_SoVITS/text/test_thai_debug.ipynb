{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\CodeProject\\tts\\GPT-SoVITS-Experiment\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/myshell-ai/MeloTTS/pull/117\n",
    "import re\n",
    "import unicodedata\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Big: Just so it can run\n",
    "# from text.symbols2 import symbols, punctuation\n",
    "# from GPT_SoVITS.text.thai_dictionary import english_dictionary, etc_dictionary\n",
    "\n",
    "\n",
    "from symbols import punctuation\n",
    "from symbols2 import symbols\n",
    "\n",
    "# from symbols2 import symbols, punctuation\n",
    "from thai_dictionary import english_dictionary, etc_dictionary\n",
    "\n",
    "from num2words import num2words\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.transliterate import romanize\n",
    "from pythainlp.util import normalize as thai_normalize\n",
    "from pythainlp.util import thai_to_eng, eng_to_thai\n",
    "\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def normalize_with_dictionary(text, dic):\n",
    "    if any(key in text for key in dic.keys()):\n",
    "        pattern = re.compile(\"|\".join(re.escape(key) for key in dic.keys()))\n",
    "        return pattern.sub(lambda x: dic[x.group()], text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize(text):\n",
    "    text = text.strip()\n",
    "    text = thai_normalize(text)\n",
    "    text = normalize_with_dictionary(text, etc_dictionary)\n",
    "    text = re.sub(r\"\\d+\", lambda x: num2words(int(x.group()), lang=\"th\"), text)\n",
    "    text = normalize_english(text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_english(text):\n",
    "\n",
    "    def fn(m):\n",
    "        word = m.group()\n",
    "        if word.upper() in english_dictionary:\n",
    "            return english_dictionary[word.upper()]\n",
    "        return \"\".join(english_dictionary.get(char.upper(), char) for char in word)\n",
    "\n",
    "    text = re.sub(r\"([A-Za-z]+)\", fn, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Load the Thai G2P dictionary\n",
    "thai_g2p_dict = defaultdict(list)\n",
    "\n",
    "dict_path = os.path.join(\"wiktionary-23-7-2022-clean.tsv\")\n",
    "with open(dict_path, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        word, phonemes = line.strip().split(\"\\t\")\n",
    "        thai_g2p_dict[word].append(phonemes.split())\n",
    "        thai_g2p_dict[\"ะ\"] = [\"a\"]\n",
    "\n",
    "\n",
    "def map_word_to_phonemes(word):\n",
    "    logger.debug(f\"Looking up word: {word}\")\n",
    "\n",
    "    # First, try to find the whole word in the dictionary\n",
    "    phonemes_list = thai_g2p_dict.get(word)\n",
    "    if phonemes_list:\n",
    "        logger.debug(f\"Found whole word {word} in dictionary\")\n",
    "        return \" \".join(phonemes_list[0])\n",
    "\n",
    "    # If not found, try to split the word\n",
    "    subwords = word_tokenize(word, engine=\"newmm\")\n",
    "\n",
    "    if len(subwords) > 1:\n",
    "        logger.debug(f\"Word {word} split into subwords: {subwords}\")\n",
    "        # If the word can be split, recursively process each subword\n",
    "        return \" . \".join(map_word_to_phonemes(subword) for subword in subwords)\n",
    "    else:\n",
    "        logger.debug(f\"Word {word} cannot be split, processing character by character\")\n",
    "        return map_partial_word(word)\n",
    "\n",
    "\n",
    "def map_partial_word(word):\n",
    "    if not word:\n",
    "        return \"\"\n",
    "\n",
    "    logger.debug(f\"Mapping partial word: {word}\")\n",
    "\n",
    "    # Handle Thanthakhat (์) character\n",
    "    if len(word) > 1 and word[1] == '์':\n",
    "        logger.debug(f\"Found Thanthakhat, skipping {word[:2]}\")\n",
    "        return map_partial_word(word[2:])\n",
    "\n",
    "    # Handle vowels and tone marks\n",
    "    if word[0] in thai_vowels or word[0] in thai_tone_marks:\n",
    "        phoneme = thai_g2p_dict.get(word[0], [word[0]])[0]\n",
    "        return phoneme + \" \" + map_partial_word(word[1:])\n",
    "\n",
    "    # Try to find the longest matching prefix\n",
    "    for i in range(len(word), 0, -1):\n",
    "        prefix = word[:i]\n",
    "        phonemes_list = thai_g2p_dict.get(prefix)\n",
    "        if phonemes_list:\n",
    "            logger.debug(f\"Found matching prefix: {prefix}\")\n",
    "            return \" \".join(phonemes_list[0]) + \" \" + map_partial_word(word[i:])\n",
    "\n",
    "    # If no match found, return the first character and continue with the rest\n",
    "    logger.debug(f\"No match found for {word[0]}, continuing with rest\")\n",
    "    return word[0] + \" \" + map_partial_word(word[1:])\n",
    "\n",
    "\n",
    "# Comprehensive mapping of Thai characters to their phonetic representations\n",
    "thai_char_to_phoneme = {\n",
    "    # Consonants\n",
    "    'ก': 'k',\n",
    "    'ข': 'kʰ',\n",
    "    'ฃ': 'kʰ',\n",
    "    'ค': 'kʰ',\n",
    "    'ฅ': 'kʰ',\n",
    "    'ฆ': 'kʰ',\n",
    "    'ง': 'ŋ',\n",
    "    'จ': 't͡ɕ',\n",
    "    'ฉ': 't͡ɕʰ',\n",
    "    'ช': 't͡ɕʰ',\n",
    "    'ซ': 's',\n",
    "    'ฌ': 't͡ɕʰ',\n",
    "    'ญ': 'j',\n",
    "    'ฎ': 'd',\n",
    "    'ฏ': 't',\n",
    "    'ฐ': 'tʰ',\n",
    "    'ฑ': 'tʰ',\n",
    "    'ฒ': 'tʰ',\n",
    "    'ณ': 'n',\n",
    "    'ด': 'd',\n",
    "    'ต': 't',\n",
    "    'ถ': 'tʰ',\n",
    "    'ท': 'tʰ',\n",
    "    'ธ': 'tʰ',\n",
    "    'น': 'n',\n",
    "    'บ': 'b',\n",
    "    'ป': 'p',\n",
    "    'ผ': 'pʰ',\n",
    "    'ฝ': 'f',\n",
    "    'พ': 'pʰ',\n",
    "    'ฟ': 'f',\n",
    "    'ภ': 'pʰ',\n",
    "    'ม': 'm',\n",
    "    'ย': 'j',\n",
    "    'ร': 'r',\n",
    "    'ล': 'l',\n",
    "    'ว': 'w',\n",
    "    'ศ': 's',\n",
    "    'ษ': 's',\n",
    "    'ส': 's',\n",
    "    'ห': 'h',\n",
    "    'ฬ': 'l',\n",
    "    'อ': 'ʔ',\n",
    "    'ฮ': 'h',\n",
    "\n",
    "    # Vowels\n",
    "    'ะ': 'a',\n",
    "    'ั': 'a',\n",
    "    'า': 'aː',\n",
    "    'ำ': 'am',\n",
    "    'ิ': 'i',\n",
    "    'ี': 'iː',\n",
    "    'ึ': 'ɯ',\n",
    "    'ื': 'ɯː',\n",
    "    'ุ': 'u',\n",
    "    'ู': 'uː',\n",
    "    'เ': 'eː',\n",
    "    'แ': 'ɛː',\n",
    "    'โ': 'oː',\n",
    "    'ใ': 'aj',\n",
    "    'ไ': 'aj',\n",
    "    '็': '',  # Short vowel marker\n",
    "    'ๆ': '',  # Repetition marker\n",
    "\n",
    "    # Tone marks\n",
    "    '่': '˨˩',  # Low tone\n",
    "    '้': '˦˥',  # Rising tone\n",
    "    '๊': '˥˩',  # Falling tone\n",
    "    '๋': '˧',  # High tone\n",
    "\n",
    "    # Special characters\n",
    "    '์': '',  # Thanthakhat (cancels sound of preceding consonant)\n",
    "}\n",
    "\n",
    "\n",
    "def map_remaining_thai_chars(phones):\n",
    "    mapped_phones = []\n",
    "    for phone in phones:\n",
    "        if phone in thai_char_to_phoneme:\n",
    "            mapped_phones.append(thai_char_to_phoneme[phone])\n",
    "        else:\n",
    "            mapped_phones.append(phone)\n",
    "    return mapped_phones\n",
    "\n",
    "\n",
    "def thai_text_to_phonemes(text):\n",
    "    text = normalize(text)\n",
    "    words = word_tokenize(text, engine=\"newmm\")\n",
    "    logger.debug(f\"word_tokenize output: {words}\")\n",
    "    phonemes = []\n",
    "    for word in words:\n",
    "        word_phonemes = map_word_to_phonemes(word)\n",
    "        phonemes.extend(word_phonemes.split())\n",
    "\n",
    "    # Map any remaining Thai characters\n",
    "    mapped_phonemes = map_remaining_thai_chars(phonemes)\n",
    "\n",
    "    return \" \".join(mapped_phonemes)\n",
    "\n",
    "\n",
    "# Define Thai vowels, tone marks, and special characters\n",
    "thai_vowels = set(\"ะัาำิีึืุูเแโใไฤฦ็\")\n",
    "thai_tone_marks = set(\"่้๊๋\")\n",
    "thai_special_chars = set(\"์ๆฯ๎๏\")  # Thanthakhat, Maiyamok, Paitai, and Phinthu\n",
    "\n",
    "# Update the thai_g2p_dict with proper mappings for vowels and tone marks\n",
    "thai_g2p_dict.update({\n",
    "    'โ': ['o'],\n",
    "    'ใ': ['aj'],\n",
    "    'ไ': ['aj'],\n",
    "    'แ': ['ɛː'],\n",
    "    'เ': ['eː'],\n",
    "    'ฤ': ['rɯ'],\n",
    "    'ฦ': ['lɯ'],\n",
    "    '็': ['ː'],  # Mai Taikhu (used to shorten a vowel)\n",
    "    '่': ['˨˩'],  # Low tone\n",
    "    '้': ['˦˥'],  # Rising tone\n",
    "    '๊': ['˥˩'],  # Falling tone\n",
    "    '๋': ['˧'],  # High tone\n",
    "    '์': [''],  # Thanthakhat (cancels the sound of the syllable)\n",
    "    'ๆ': [''],  # Maiyamok (repetition mark)\n",
    "    'ฯ': [''],  # Paitai (abbreviation mark)\n",
    "    '๎': [''],  # Phinthu (used to indicate a silent consonant)\n",
    "    '๏': [''],  # Angkhankhu (used to mark the end of a paragraph or section)\n",
    "})\n",
    "\n",
    "\n",
    "def text_normalize(text):\n",
    "    text = normalize(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def distribute_phone(n_phone, n_word):\n",
    "    phones_per_word = [0] * n_word\n",
    "    for task in range(n_phone):\n",
    "        min_tasks = min(phones_per_word)\n",
    "        min_index = phones_per_word.index(min_tasks)\n",
    "        phones_per_word[min_index] += 1\n",
    "    return phones_per_word\n",
    "\n",
    "\n",
    "model_id = 'clicknext/phayathaibert'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "tone_map = {\n",
    "    \"˧\": 2,  # Mid tone\n",
    "    \"˨˩\": 1,  # Low tone\n",
    "    \"˦˥\": 3,  # Rising tone\n",
    "    \"˩˩˦\": 4,  # Falling tone\n",
    "    \"˥˩\": 5,  # High tone\n",
    "}\n",
    "\n",
    "# def extract_tones(phs):\n",
    "#     tones = []\n",
    "#     tone_value = 2  # Default tone value when no tone symbol is found\n",
    "#     last_item = phs[-1]\n",
    "\n",
    "#     for ph in phs:\n",
    "#         if ph in tone_map:\n",
    "#             tone_value = tone_map[ph]\n",
    "\n",
    "#     # Assign the tone value to each phoneme excluding the tone marker if there is one\n",
    "#     if last_item in tone_map:\n",
    "#         tones = [tone_value] * (len(phs) - 1)\n",
    "#     else:\n",
    "#         tones = [tone_value] * (len(phs))\n",
    "\n",
    "#     print(\"=======> PHS: \",  phs)\n",
    "#     print(\"=======> TONES: \",  tones)\n",
    "\n",
    "#     return tones\n",
    "\n",
    "\n",
    "def extract_tones(phs):\n",
    "    phonemes = []\n",
    "    tones = []\n",
    "    current_tone = 2  # Default mid tone\n",
    "    for ph in phs.split():\n",
    "        if ph in tone_map:\n",
    "            current_tone = tone_map[ph]\n",
    "        elif ph == '_':\n",
    "            phonemes.append(ph)\n",
    "            tones.append(0)  # Zero tone for underscore\n",
    "        else:\n",
    "            phonemes.append(ph)\n",
    "            tones.append(current_tone)\n",
    "    return phonemes, tones\n",
    "\n",
    "\n",
    "# def g2p_bert(norm_text, pad_start_end=True):\n",
    "#     tokenized = tokenizer.tokenize(norm_text)\n",
    "#     print(\"tokenized text\")\n",
    "#     phs = []\n",
    "#     tones = []\n",
    "#     word2ph = []\n",
    "\n",
    "#     ph_groups = []\n",
    "#     for t in tokenized:\n",
    "#         if t.startswith(\"▁\"):\n",
    "#             ph_groups.append([t])\n",
    "#         else:\n",
    "#             ph_groups[-1].append(t)\n",
    "\n",
    "#     for group in ph_groups:\n",
    "#         word = \"\".join(group).replace(\"▁\", \"\")\n",
    "#         phonemes = thai_text_to_phonemes(word)\n",
    "#         phoneme_groups = phonemes.split(\".\")\n",
    "#         phoneme_groups = list(filter(str.strip, phoneme_groups))\n",
    "\n",
    "#         word_phonemes = []\n",
    "#         word_tones = []\n",
    "#         word_phones_count = []\n",
    "\n",
    "#         for p_group in phoneme_groups:\n",
    "#             group_phonemes = [ph for ph in p_group.split() if ph not in tone_map]\n",
    "#             group_tones = extract_tones(p_group.split())\n",
    "\n",
    "#             word_phonemes.extend(group_phonemes)\n",
    "#             word_tones.extend(group_tones)\n",
    "#             word_phones_count.append(len(group_phonemes))\n",
    "\n",
    "#         phs.extend(word_phonemes)\n",
    "#         tones.extend(word_tones)\n",
    "#         word2ph.extend(word_phones_count)\n",
    "\n",
    "#     if pad_start_end:\n",
    "#         phs = [\"_\"] + phs + [\"_\"]\n",
    "#         tones = [1] + tones + [1]\n",
    "#         word2ph = [1] + word2ph + [1]\n",
    "\n",
    "#     print(f\"Final phs: {phs}\")\n",
    "#     print(f\"Final tones: {tones}\")\n",
    "#     print(f\"Final word2ph: {word2ph}\")\n",
    "\n",
    "#     assert len(word2ph) == len(tokenized) + 2\n",
    "\n",
    "#     return phs, tones, word2ph\n",
    "\n",
    "\n",
    "def g2p_og(norm_text, pad_start_end=True):\n",
    "    tokenized = tokenizer.tokenize(norm_text)\n",
    "    phs = []\n",
    "    tones = []\n",
    "    word2ph = []\n",
    "\n",
    "    print(\"tokenized\", tokenized)\n",
    "\n",
    "    for word in tokenized:\n",
    "        # if word == \"▁\":\n",
    "        #     word2ph.append(1)\n",
    "        #     continue\n",
    "\n",
    "        if word.startswith(\"▁\"):\n",
    "            word = word.replace(\"▁\", \"\")\n",
    "\n",
    "        phonemes = thai_text_to_phonemes(word)\n",
    "        phoneme_groups = phonemes.split(\".\")\n",
    "        # Keep only non-empty groups for cases with a trailing dot\n",
    "        # i.e 'b ɤː ˧ . tʰ oː ˧ . r a ˦˥ .'\n",
    "        phoneme_groups = [group for group in phoneme_groups if group.strip()]\n",
    "\n",
    "        word_phonemes = []\n",
    "        word_tones = []\n",
    "\n",
    "        for group in phoneme_groups:\n",
    "            group_phonemes = [ph for ph in group.split() if ph not in tone_map]\n",
    "            group_tones = extract_tones(group.split())\n",
    "            word_phonemes.extend(group_phonemes)\n",
    "            word_tones.extend(group_tones)\n",
    "\n",
    "        phs.extend(word_phonemes)\n",
    "        tones.extend(word_tones)\n",
    "        word2ph.append(len(word_phonemes))\n",
    "\n",
    "    if pad_start_end:\n",
    "        phs = [\"_\"] + phs + [\"_\"]\n",
    "        tones = [1] + tones + [1]\n",
    "        word2ph = [1] + word2ph + [1]\n",
    "\n",
    "    print(f\"Final phs: {phs}\")\n",
    "    print(f\"Final tones: {tones}\")\n",
    "    print(f\"Final word2ph: {word2ph}\")\n",
    "\n",
    "    # assert len(word2ph) == len([t for t in tokenized if t != \"▁\"]) + 2\n",
    "\n",
    "    return phs, tones, word2ph\n",
    "\n",
    "\n",
    "def g2p_no_undersscores(norm_text, pad_start_end=True):\n",
    "    tokenized = tokenizer.tokenize(norm_text)\n",
    "    # print(\"The tokenized text\", tokenized)\n",
    "    phs = []\n",
    "    tones = []\n",
    "    ph_groups = []\n",
    "\n",
    "    # Group tokens\n",
    "    for t in tokenized:\n",
    "        if not t.startswith(\"#\"):\n",
    "            ph_groups.append([t])\n",
    "        else:\n",
    "            ph_groups[-1].append(t.replace(\"#\", \"\"))\n",
    "\n",
    "    word2ph = []\n",
    "    for group in ph_groups:\n",
    "        text = \"\".join(group)\n",
    "\n",
    "        # Special token handling\n",
    "        if text == '[UNK]':\n",
    "            phs += ['_']\n",
    "            tones += [0]\n",
    "            word2ph += [1]\n",
    "            continue\n",
    "        elif text in punctuation:\n",
    "            phs += [text]\n",
    "            tones += [0]\n",
    "            word2ph += [1]\n",
    "            continue\n",
    "\n",
    "        # Phoneme conversion for grouped text\n",
    "        phonemes = thai_text_to_phonemes(text)\n",
    "        phoneme_groups = phonemes.split(\".\")\n",
    "        phoneme_groups = list(filter(str.strip, phoneme_groups))\n",
    "\n",
    "        word_phonemes = []\n",
    "        word_tones = []\n",
    "        for p_group in phoneme_groups:\n",
    "            group_phonemes = [ph for ph in p_group.split() if ph not in tone_map]\n",
    "            group_tones = extract_tones(p_group.split())\n",
    "            word_phonemes.extend(group_phonemes)\n",
    "            word_tones.extend(group_tones)\n",
    "\n",
    "        phone_len = len(word_phonemes)\n",
    "        word_len = len(group)\n",
    "\n",
    "        aaa = distribute_phone(phone_len, word_len)\n",
    "        assert len(aaa) == word_len\n",
    "        word2ph += aaa\n",
    "\n",
    "        phs += word_phonemes\n",
    "        tones += word_tones\n",
    "\n",
    "    if pad_start_end:\n",
    "        phs = [\"_\"] + phs + [\"_\"]\n",
    "        tones = [0] + tones + [0]\n",
    "        word2ph = [1] + word2ph + [1]\n",
    "\n",
    "    # print(f\"Final phs: {phs}\")\n",
    "    # print(f\"Final tones: {tones}\")\n",
    "    # print(f\"Final word2ph: {word2ph}\")\n",
    "\n",
    "    assert len(word2ph) == len(tokenized) + 2\n",
    "    return phs, tones, word2ph\n",
    "\n",
    "\n",
    "def g2p(norm_text, pad_start_end=True):\n",
    "    tokenized = tokenizer.tokenize(norm_text)\n",
    "    phs = []\n",
    "    # tones = []\n",
    "    ph_groups = []\n",
    "\n",
    "    # Group tokens\n",
    "    for t in tokenized:\n",
    "        if t == '▁':\n",
    "            ph_groups.append([t])  # Add '▁' as its own group\n",
    "        else:\n",
    "            if not ph_groups or ph_groups[-1] == ['▁']:\n",
    "                ph_groups.append([t])\n",
    "            else:\n",
    "                ph_groups[-1].append(t)\n",
    "\n",
    "    word2ph = []\n",
    "    word_phonemes = []\n",
    "    for group in ph_groups:\n",
    "        text = \"\".join(group)\n",
    "        if text == '▁':\n",
    "            phs += ['_']\n",
    "            # tones += [0]  # Zero tone for underscore\n",
    "            word2ph += [1]\n",
    "            continue\n",
    "        elif text == '[UNK]':\n",
    "            phs += ['_']\n",
    "            # tones += [0]  # Zero tone for unknown token\n",
    "            word2ph += [1]\n",
    "            continue\n",
    "        elif text in punctuation:\n",
    "            phs += [text]\n",
    "            # tones += [0]  # Zero tone for punctuation\n",
    "            word2ph += [1]\n",
    "            continue\n",
    "\n",
    "        # Phoneme conversion for grouped text\n",
    "        phonemes = thai_text_to_phonemes(text)\n",
    "        phoneme_groups = phonemes.split(\".\")\n",
    "        phoneme_groups = list(filter(str.strip, phoneme_groups))\n",
    "\n",
    "        word_phonemes = []\n",
    "        # word_tones = []\n",
    "        for p_group in phoneme_groups:\n",
    "            group_phonemes, group_tones = extract_tones(p_group)\n",
    "            word_phonemes.extend(group_phonemes)\n",
    "            # word_tones.extend(group_tones)\n",
    "\n",
    "        phone_len = len(word_phonemes)\n",
    "        word_len = len(group)\n",
    "        aaa = distribute_phone(phone_len, word_len)\n",
    "        # assert len(aaa) == word_len\n",
    "        word2ph += aaa\n",
    "        # phs += word_phonemes\n",
    "        # tones += word_tones\n",
    "\n",
    "    if pad_start_end:\n",
    "        phs = [\"_\"] + phs + [\"_\"]\n",
    "        # tones = [0] + tones + [0]  # Zero tone for start/end padding\n",
    "        word2ph = [1] + word2ph + [1]\n",
    "\n",
    "    # assert len(phs) == len(tones)\n",
    "    # assert len(phs) == sum(word2ph)\n",
    "\n",
    "    # return phs, tones, word2ph\n",
    "\n",
    "    return word_phonemes, word2ph\n",
    "\n",
    "def get_bert_feature(text, word2ph, device='cuda', model_id='clicknext/phayathaibert'):\n",
    "    from thai_bert import get_bert_feature\n",
    "    return get_bert_feature(text, word2ph, device=device, model_id=model_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp_add.list','r', encoding='utf-8')as f:\n",
    "    lines = f.read().strip().split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word  = [line.split('|')[-1] for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from symbols import symbols\n",
    "# text = \"ฉันเข้าใจคุณค่าของงานของฉันและความหมายของสิ่งที่ฟอนเทนทำเพื่อคนทั่วไปเป็นอย่างดี ฉันจะใช้ชีวิตอย่างภาคภูมิใจในงานของฉันต่อไป\"\n",
    "\n",
    "new_symbols = []\n",
    "\n",
    "# bert = get_bert_feature(text, word2ph, device='cuda', model_id=model_id)\n",
    "\n",
    "for text in word:\n",
    "    text = text_normalize(text)\n",
    "    phones, word2ph = g2p(text)\n",
    "    for ph in phones:\n",
    "        if ph not in symbols and ph not in new_symbols:\n",
    "            new_symbols.append(ph)\n",
    "            print('update!, new symbols:')\n",
    "            print(new_symbols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "new_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ถ้า ร่างรัฐธรรมนูญ ผ่าน เราจะมีแพทย์ประจำครอบครัวดูแล บัตรทอง ก็ยังอยู่จริงไหม อยากรู้คำตอบ'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['j', 'aː', 'k̚', 'r', 'uː', 'kʰ', 'a', 'm', 't', 'ɔː', 'p̚']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
